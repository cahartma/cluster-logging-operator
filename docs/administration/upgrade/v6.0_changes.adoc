:toc:
:toclevels: 4

= What's New In Logging 6.0

== Overview
Logging version 6.0 is a major change from earlier releases and is the realization of several longstanding goals.

The main highlights of these changes are:

* There are now distinct operators to support each separate logging components (e.g. collectors, storage, visualization)
* The ClusterLoggingOperator no longer manages log storage or visualization of any kind including LokiStack resource or Elastic products (i.e. Elasticsearch, Kibana)
* CLO has removed support of Fluentd log collector implementation
* Introduce the `ClusterLogForwarder.observability.openshift.io` API to replace both `ClusterLogging.logging.openshift.io` and `ClusterLogForwarder.logging.openshift.io` APIs

.5.x
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
----
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
----

Replaced with:

.6.0
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
----

NOTE: There is no automated upgrade provided by the *cluster-logging-operator*

Given the numerous combinations in which log collection, forwarding, and storage can be configured, there is no automated upgrade provided by the *cluster-logging-operator*.  The following documentation is intended to assist administrators in converting exising **ClusterLogging.logging.openshift.io** and **ClusterLogForwarder.logging.openshift.io** specifications to the new API.  This document includes example of migrated **ClusterLogForwarder.observability.openshift.io** resources for several common use cases.

== Changes

Cluster Logging no longer provides a "one click" installation of a complete logging solution in favor of administrators
having more granular control over individual components.  This means administrators must explicitly deploy an operator to control
a given component. The general steps for deploying a complete logging solution are:

1. Deploy the Red Hat **cluster-observability-operator**
2. Deploy the Red Hat **loki-operator**
3. Create an instance of **LokiStack** in the *openshift-logging* namespace
4. Deploy the Red Hat **cluster-logging-operator**
5. Create an instance of the **ClusterLogForwarder.observability.openshift.io** resource

=== Log Storage
The only available managed log storage solution for this release is a Loki stack that is based upon the **loki-operator**.  This
solution was available in prior releases as the preferred alternative to the managed Elasticsearch offering.  The deployment
of this solution remains unchanged from previous releases. Read the https://docs.openshift.com/container-platform/4.16/observability/logging/log_storage/installing-log-storage.html[official] product documentation for more information.

NOTE: To continue to use an existing Red Hat managed Elasticsearch deployment provided by the **elasticsearch-operator**,
remove the owner references from the **Elasticsearch** resource named '**elasticsearch**' in the '**openshift-logging**'
namespace before removing the **ClusterLogging** resourced named '**instance**' in the '**openshift-logging**' namespace

=== Log Visualization
The OpenShift console UI plugin that provides visualization was moved to the **cluster-observability-operator** from the
**cluster-logging-operator**. Read the https://docs.openshift.com/container-platform/4.16/observability/cluster_observability_operator/installing-the-cluster-observability-operator.html[official] product documentation
for more information.

NOTE: To continue to use an existing Red Hat managed Kibana deployment provided by the **elasticsearch-operator**,
remove the owner references from the **Kibana** resource named '**kibana**' in the '**openshift-logging**'
namespace before removing the **ClusterLogging** resourced named '**instance**' in the '**openshift-logging**' namespace

=== Log Collection & Forwarding

Log collection and forwarding configuration is spec'd from a new link:../../reference/operator/api_observability_v1.adoc[API]
that is included in the API group **observability.openshift.io**. The following sections highlight the differences from the
https://github.com/openshift/cluster-logging-operator/blob/release-5.9/docs/reference/operator/api.adoc[old API] resource.

NOTE: Vector is the only supported collector implementation.

==== Permissions

This release of Cluster Logging requires a service account to be specified.  Administrators must now *explicitly grant log collection permissions* to the service account associated with *ClusterLogForwarder*.  This was not required in previous releases for the legacy logging scenario consisting of a *ClusterLogging* and, optionally, a *ClusterLogForwarder.logging.openshift.io* resource.

[source, yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: my-forwarder
spec:
  serviceAccount:
    name: logcollector
...
----
Using the existing service account (i.e. *logcollector*) from a previous release requires creating the following *ClusterRoleBinding*:

----
oc adm policy add-cluster-role-to-user collect-application-logs -z logcollector
oc adm policy add-cluster-role-to-user collect-infrastructure-logs -z logcollector
----

Additionally, create the following *ClusterRoleBinding* if collecting audit logs:

----
oc adm policy add-cluster-role-to-user collect-audit-logs -z logcollector
----

==== Management, Resource Allocation & Workload Scheduling
Configuration of the management state (i.e. managed, unmanaged), resource request and limits, tolerations, and node selection
are part of the new ClusterLogForwarder API.

.Previously part of the resource *ClusterLogging* (v5.9)
[source, yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging
metadata:
  name: instance
spec:
  managementState: Managed
  collection:
    resources:
      limits: {}
      requests: {}
    nodeSelector: {}
    tolerations: {}
----
.Now part of v6.0 *ClusterLogForwarder* along with inputs, outputs, filters and pipelines
[source, yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
spec:
  managementState: Managed
  collector:
    resources:
      limits: {}
      requests: {}
    nodeSelector: {}
    tolerations: {}
...
----

==== Inputs Specifications

The input spec is an optional part of the *ClusterLogForwarder* spec where administrators can continue to use the pre-defined values of *application*, *infrastructure*, and *audit* to collect those sources.
See the https://github.com/openshift/cluster-logging-operator/blob/master/docs/reference/operator/api_observability_v1.adoc#specinputs[Input Spec] document for definitions of these values.
The spec, otherwise, has largely remained unchanged.

===== Application Inputs
Simplified namespace and container inclusion and exclusions are now collapsed into a single field

.v5.9 (Previous)
[source, yaml]
----
...
spec:
  inputs:
  - name: app-logs
    type: application
    application:
      namespaces:
      - foo
      - bar
      includes:
      - namespace: my-important
        container: main
      excludes:
      - container: too-verbose
...
----

.v6.0 Simplified Grouping of Includes and Excludes (New)
[source, yaml]
----
...
spec:
  inputs:
  - name: app-logs
    type: application
    application:
      includes:
      - namespace: foo
      - namespace: bar
      - namespace: my-important
        container: main
      excludes:
      - container: too-verbose
...
----

NOTE: *application*, *infrastructure*, and *audit* are reserved words and can not be used for the name when defining an input

===== Input Receivers

Input receivers now require explicit configuration of the `type` and `port` at the receiver level

.v5.9 (Previous)
[source, yaml]
----
...
spec:
  inputs:
  - name: an-http
    receiver:
      http:
        port: 8443
        format: kubeAPIAudit
  - name: a-syslog
    receiver:
      type: syslog
      syslog:
        port: 9442
...
----

.v6.0 Explicit Type and Port (New)
[source, yaml]
----
...
spec:
  inputs:
  - name: an-http
    type: receiver
    receiver:
      type: http
      port: 8443
      http:
        format: kubeAPIAudit
  - name: a-syslog
    type: receiver
    receiver:
      type: syslog
      port: 9442
...
----

==== Output Specifications

The high-level output spec changes:

* Moves URL to each output type spec
* Moves tuning to each output type spec
* Separates TLS from authentication
* Requires explicit configuration of keys and secret/configmap for TLS and authentication

==== Secrets & TLS Configuration
Secrets and TLS configuration are separated into `authentication` and `tls` configuration for each output.
They are now explicitly defined instead of relying upon administrators to specify secrets with recognized https://github.com/openshift/cluster-logging-operator/blob/release-5.9/docs/reference/operator/secrets.adoc[keys].

NOTE: The new configuration requires administrators to understand the previously recognized keys in order to continue to use the existing secrets.

.v6.0 Output Authentication and TLS Example
[source, yaml]
----
...
spec:
  outputs:
  - name: my-output
    type: http
    http:
      url: https://my-secure-output:8080
    authentication:
      password:
        key: pass
        secretName: my-secret
      username:
        key: user
        secretName: my-secret
    tls:
      ca:
        key: ca-bundle.crt
        secretName: collector
      certificate:
        key: tls.crt
        secretName: collector
      key:
        key: tls.key
        secretName: collector
...
----

.v6.0 Authentication using ServiceAccount token
[source,yaml]
----
...
spec:
  outputs:
  - name: my-output
    type: http
    http:
      url: https://my-secure-output:8080
    authentication:
      token:
        from: serviceAccount
    tls:
      ca:
        key: service-ca.crt
        configMapName: openshift-service-ca.crt
...
----

==== Filters & Pipeline Configuration

All attributes of pipelines in previous releases have been converted to filters in this release.
Individual filters are defined in the "filters" spec and referenced by a pipeline

.v5.9 Filters (Previous)
[source, yaml]
----
...
spec:
  pipelines:
  - name: app-logs
    detectMultilineErrors: true
    parse: json
    labels:
      foo: bar
...
----

.v6.0 Filter and Pipeline Spec (New)
[source, yaml]
----
...
spec:
  filters:
  - name: my-multiline
    type: detectMultilineException
  - name: my-parse
    type: parse
  - name: my-labels
    type: openshiftLabels
    openshiftLabels:
      foo: bar
  pipelines:
  - name: app-logs
    filterRefs:
    - my-multiline
    - my-parse
    - my-labels
...
----
NOTE: Drop filter, Prune filter and KubeAPIAudit filters remain unchanged

[source, yaml]
----
...
spec:
  filters:
  - name: drop-debug-logs
    type: drop
    drop:
    - test:
      - field: .level
        matches: debug
  - name: prune-fields
    type: prune
    prune:
      in:
      - .kubernetes.labels.foobar
      notIn:
      - .message
  - name: audit-logs
    type: kubeAPIAudit
    kubeAPIAudit:
      omitResponseCodes:
      - 404
      - 409
...
----


==== Validation & Status
Most validations are now enforced when a resource is created or updated which provides immediate feedback.  This is
a departure from previous releases where all validation occurred post creation requiring inspection of the resource status location.  Some validation still occurs post resource creation for cases where is not possible to do so at creation or update time.

Instances of the **ClusterLogForwarder.observability.openshift.io** must satisfy the following before
the operator will deploy the log collector:

- *Resource Status Conditions:* `Authorized, Valid, Ready`

- *Spec Validations:* `Filters, Inputs, Outputs, Pipelines`

All must evaluate to `status: "True"`

.v6.0 Status "True" Conditions Example
[source, yaml]
----
...
status:
  conditions:
  - message: "permitted to collect log types: [application]"
    reason: ClusterRoleExists
    status: "True"
    type: observability.openshift.io/Authorized
  - message: ""
    reason: ValidationSuccess
    status: "True"
    type: observability.openshift.io/Valid
  - message: ""
    status: "True"
    type: observability.openshift.io/Ready
  filterConditions:
  - message: filter "my-parse" is valid
    reason: ValidationSuccess
    status: "True"
    type: observability.openshift.io/ValidFilter-my-parse
  inputConditions:
  - message: input "application" is valid
    reason: ValidationSuccess
    status: "True"
    type: observability.openshift.io/ValidInput-application
  outputConditions:
  - message: output "rh-loki" is valid
    reason: ValidationSuccess
    status: "True"
    type: observability.openshift.io/ValidOutput-rh-loki
  pipelineConditions:
  - message: pipeline "app-logs" is valid
    reason: ValidationSuccess
    status: "True"
    type: observability.openshift.io/ValidPipeline-app-logs
...
----

NOTE: Conditions that have a "status" other than "True" will provide information identifying the failure.

.6.0 Status "False" Example
[source, yaml]
----
...
status:
  conditions:
  - message: insufficient permissions on service account, not authorized to collect 'application' logs
    reason: ClusterRoleMissing
    status: "False"
    type: observability.openshift.io/Authorized
  - message: ""
    reason: ValidationFailure
    status: "False"
    type: Ready
...
----

== Examples & Common Use Cases

=== Forwarding to CloudWatch
====
.Complete spec using long-lived static credentials from a secret
[source, yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: my-forwarder
spec:
  serviceAccount:
    name: my-account
  outputs:
  - name: my-cw
    type: cloudwatch
    cloudwatch:
      groupName: my-cluster-{.log_type||"unknown"}
      region: us-east-1
      authentication:
        type: awsAccessKey
        awsAccessKey:
          keyId:
            secretName: cw-secret
            key: aws_access_key_id
          keySecret:
            secretName: cw-secret
            key: aws_secret_access_key
  pipelines:
  - name: my-cw-logs
    inputRefs:
      - application
      - infrastructure
    outputRefs:
      - my-cw
----
.Alternative CW Auth snippet using short-lived token (SA Token)
[source, yaml]
----
...
    cloudwatch:
      authentication:
        type: iamRole
        iamRole:
          roleARN:
            secretName: role-for-sts
            key: credentials
          token:
            from: serviceAccount
...
----
.Alternative CW Auth snippet using role and static token (Self-Generated Token)
[source, yaml]
----
...
    cloudwatch:
      authentication:
        type: iamRole
        iamRole:
          roleARN:
            secretName: role-for-sts
            key: credentials
          token:
            from: secret
            secret:
              key: token
              name: cw-token
...
----
====


=== Forwarding to Red Hat Managed LokiStack
====
.Complete spec using service account authentication and tls
[source,yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: my-forwarder
spec:
  serviceAccount:
    name: my-user
  outputs:
  - name: default-lokistack
    type: lokiStack
    lokiStack:
      target:
        name: logging-loki
        namespace: openshift-logging
      authentication:
        token:
          from: serviceAccount
    tls:
      ca:
        key: service-ca.crt
        configMapName: openshift-service-ca.crt
  pipelines:
  - name: my-pipeline
    outputRefs:
    - default-lokistack
    inputRefs:
    - application
    - infrastructure
    - audit
----
====


=== Forwarding to External Elasticsearch
====
.Complete spec including url, version and custom index
[source, yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: my-forwarder
spec:
  serviceAccount:
    name: my-user
  outputs:
  - name: es-external
    type: elasticsearch
    elasticsearch:
      url: https://external-es-service:9200
      version: 8
      index: '{.log_type||"nologformat"}-write'
    tls:
      ca:
        key: bundle.crt
        secretName: my-tls-secret
      certificate:
        key: tls.crt
        secretName: my-tls-secret
      key:
        key: tls.key
        secretName: my-tls-secret
  filters:
  - name: my-parse
    type: parse
  pipelines:
  - name: my-pipeline
    inputRefs:
    - application
    - infrastructure
    filterRefs:
    - my-parse
    outputRefs:
    - es-external
----
`index` can be a combination of dynamic and static values. Dynamic values are enclosed in curly brackets `{}`
and MUST end with a "quoted" static fallback value separated with `||`.

More details use: `oc explain clf.spec.outputs.elasticsearch.index`

NOTE: In this example, application logs are written to the 'application-write' and 'infrastructure-write' index.
Previous versions without the `index` spec, would have instead written to 'app-write' and 'infra-write'.
====


=== Forwarding to Red Hat Managed Elasticsearch
====
.Complete spec including url, version and index values from labels
[source, yaml]
----
apiVersion: observability.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: my-forwarder
spec:
  serviceAccount:
    name: logcollector
  outputs:
  - name: es-output-by-label # <1>
    type: elasticsearch
    elasticsearch:
      url: https://elasticsearch:9200
      version: 6
      index: '{.openshift.label.my-log-index||"app"}-write' # <2>
    tls:
      ca:
        key: ca-bundle.crt
        secretName: collector
      certificate:
        key: tls.crt
        secretName: collector
      key:
        key: tls.key
        secretName: collector
  filters:
  - name: my-parse
    type: parse
  - name: my-app-label # <3>
    type: openshiftLabels
    openshiftLabels:
      my-log-index: app
  - name: my-infra-label # <4>
    type: openshiftLabels
    openshiftLabels:
      my-log-index: infra
  pipelines:
  - name: my-app # <5>
    inputRefs:
    - application
    filterRefs:
    - my-parse
    - my-app-label
    outputRefs:
    - es-output-by-label
  - name: my-infra # <6>
    inputRefs:
    - infrastructure
    filterRefs:
    - my-parse
    - my-infra-label
    outputRefs:
    - es-output-by-label
----
. `es-output-by-label` is the output used in both pipelines
. `index` is set to read the value from `.openshift.label.my-log-index` and prepend to the string "-write" or fallback to "app-write"
. `my-app-label` filter is used to set the label "my-log-index:app" in the pipeline
. `my-infra-label` filter is used to set the label "my-log-index:infra" in the pipeline
. pipeline `my-app` includes application logs and labels them `app`
. pipeline `my-infra` includes infrastructure logs and labels them `infra`

NOTE: In order to forward logs to the default RH-managed Elasticsearch, the `index` values must be one of `app-write`, `infra-write` or `audit-write`.
This is achieved by adding a label (filter) to each pipeline, and setting the label value to the corresponding input type.
====

=== Additional info on ES Custom Index
====
Custom ES indices in v5.9 was achieved via `structuredTypeKey` and `structuredTypeName` options

.v5.9 Snippet (Previous)
[source, yaml]
----
...
spec:
  outputs:
    - name: default
      type: elasticsearch
      elasticsearch:
        structuredTypeKey: kubernetes.namespace_name
        structuredTypeName: nologformat
...
----
.v6.0 Custom Index Snippet (New)
[source, yaml]
----
...
spec:
  outputs:
  - name: es-output
    type: elasticsearch
    elasticsearch:
      url: https://elasticsearch:9200
      version: 6
      index: '{ .kubernetes.namespace_name || "nologformat" }' # <1>
...
----
. `index` is set to read the field value `.kubernetes.namespace_name` and falls back to "nologformat" if not found

====